{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所有模型的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V wqqpython\n",
    "plt.rcParams[\"font.sans-serif\"] = ['Simhei']\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据的处理\n",
    "* 过滤掉所有特殊字符，只保留所有中文字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process():  # 数据预处理函数\n",
    "    label_list = []\n",
    "    text_file_list = []\n",
    "    with open('label/index', encoding='utf-8') as f:\n",
    "        f = f.read().splitlines()[:10000]\n",
    "        for i in f:\n",
    "            if i.split(\" \")[0] == 'spam':\n",
    "                label_list.append(0)\n",
    "            else:\n",
    "                label_list.append(1)\n",
    "            text_file_list.append(i.split(\" \")[1][3:])\n",
    "    data = []\n",
    "    for file_path in text_file_list:\n",
    "        with open(file_path, errors='ignore', encoding='utf-8') as f:\n",
    "            text = f.readlines()\n",
    "            res = re.findall('[\\u4e00-\\u9fa5]', str(text))\n",
    "            res = \"\".join(res)\n",
    "            data.append(res)\n",
    "    return data, label_list\n",
    "\n",
    "# 获取训练集的文本和标签\n",
    "train_text_data, train_text_data_label = data_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用TF-IDF来进行分词处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用Word2Vec来进行分词处理\n",
    "* 和TF-IDF形成对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用word2vec之前先进行word2vec的语料库训练 只需要训练一次 就可以\n",
    "with open(\"word2vec_txt.txt\", \"a+\", encoding='utf-8') as f:\n",
    "    words = []\n",
    "    for i in tqdm(train_text_data):\n",
    "        i = \"\".join(re.findall('[\\u4e00-\\u9fa5]', str(i)))\n",
    "        i = \" \".join(list(jieba.cut(i, cut_all=False)))\n",
    "        f.write(i)\n",
    "        f.write(\"\\n\")\n",
    "model = Word2Vec(LineSentence(open('word2vec_txt.txt', 'r', encoding='utf-8')), sg=0, vector_size=64, window=3,\n",
    "                 min_count=3, workers=4)\n",
    "# 模型保存\n",
    "model.save('test.model')\n",
    "\n",
    "# 通过模型加载词向量(recommend)\n",
    "model = gensim.models.Word2Vec.load('test.model')\n",
    "dic = model.wv.index_to_key\n",
    "# print(dic)\n",
    "# print(len(dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_text_data, train_text_data_label, test_size=0.2)\n",
    "train_vec = []\n",
    "train_vec_label = []\n",
    "test_vec = []\n",
    "test_vec_label = []\n",
    "\n",
    "# 对每一个向量取均值，用来作为一段正文文本的向量值\n",
    "for idx, line in enumerate(x_train):\n",
    "    vec = np.zeros(64).reshape((1, 64))\n",
    "    count = 0\n",
    "    for word in jieba.cut(line, cut_all=False):\n",
    "        try:\n",
    "            vec = vec + model.wv[word].reshape((1, 64))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    train_vec.append(vec[0])\n",
    "    train_vec_label.append(y_train[idx])\n",
    "\n",
    "\n",
    "for idx, line in enumerate(x_test):\n",
    "    vec = np.zeros(64).reshape((1, 64))\n",
    "    count = 0\n",
    "    for word in jieba.cut(line, cut_all=False):\n",
    "        try:\n",
    "            vec = vec + model.wv[word].reshape((1, 64))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    test_vec.append(vec[0])\n",
    "    test_vec_label.append(y_test[idx])\n",
    "\n",
    "print(train_vec[:10])\n",
    "print('*************************************************************')\n",
    "print(test_vec[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林机器学习算法来进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('使用随机森林模型')\n",
    "\n",
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(train_vec, y_train)\n",
    "\n",
    "joblib.dump(random_forest_model, 'random_forest.pkl', compress=3)\n",
    "y_pred = random_forest_model.predict(test_vec)\n",
    "# print(type(y_pred), len(y_pred), y_pred)\n",
    "# print(type(y_test), len(y_test), y_test)\n",
    "\n",
    "random_forest_accuracy = accuracy_score(y_test, y_pred)\n",
    "random_forest_precision = precision_score(y_test, y_pred, average='macro')\n",
    "random_forest_recall = recall_score(y_test, y_pred, average='macro')\n",
    "random_forest_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {random_forest_accuracy}')\n",
    "print(f'precision: {random_forest_precision}')\n",
    "print(f'Recall: {random_forest_recall}')\n",
    "print(f'F1 Score: {random_forest_f1}')\n",
    "print(random_forest_model.predict_proba(test_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_probas = clf.predict_proba(test_vec)[:, 1]\n",
    "# fpr, tpr, _ = roc_curve(y_test, pred_probas)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.plot(fpr, tpr, label='area = %.2f' % roc_auc)\n",
    "# # 保存到csv文件\n",
    "# df_tmp = pd.DataFrame({'fpr': fpr, 'tpr': tpr, })\n",
    "# df_tmp.to_csv('plot.csv', index=False, encoding='utf_8_sig')\n",
    "# # 绘制折线图\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.savefig('ROC曲线.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print('使用决策树模型')\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(train_vec, y_train)\n",
    "\n",
    "joblib.dump(decision_tree_model, 'decision_tree.pkl', compress=3)\n",
    "y_pred = decision_tree_model.predict(test_vec)\n",
    "# print(type(y_pred), len(y_pred), y_pred)\n",
    "# print(type(y_test), len(y_test), y_test)\n",
    "\n",
    "decision_tree_accuracy = accuracy_score(y_test, y_pred)\n",
    "decision_tree_precision = precision_score(y_test, y_pred, average='macro')\n",
    "decision_tree_recall = recall_score(y_test, y_pred, average='macro')\n",
    "decision_tree_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {decision_tree_accuracy}')\n",
    "print(f'precision: {decision_tree_precision}')\n",
    "print(f'Recall: {decision_tree_recall}')\n",
    "print(f'F1 Score: {decision_tree_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用支持向量机模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "print('使用支持向量机模型')\n",
    "\n",
    "SVC_model = SVC(probability=True)\n",
    "SVC_model.fit(train_vec, y_train)\n",
    "\n",
    "joblib.dump(SVC_model, 'SVC.pkl', compress=3)\n",
    "y_pred = SVC_model.predict(test_vec)\n",
    "# print(type(y_pred), len(y_pred), y_pred)\n",
    "# print(type(y_test), len(y_test), y_test)\n",
    "\n",
    "SVC_accuracy = accuracy_score(y_test, y_pred)\n",
    "SVC_precision = precision_score(y_test, y_pred, average='macro')\n",
    "SVC_recall = recall_score(y_test, y_pred, average='macro')\n",
    "SVC_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {SVC_accuracy}')\n",
    "print(f'precision: {SVC_precision}')\n",
    "print(f'Recall: {SVC_recall}')\n",
    "print(f'F1 Score: {SVC_f1}')\n",
    "print(SVC_model.predict_proba(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print('使用朴素贝叶斯模型')\n",
    "\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(train_vec, y_train)\n",
    "\n",
    "joblib.dump(naive_bayes_model, 'naive_bayes.pkl', compress=3)\n",
    "y_pred = naive_bayes_model.predict(test_vec)\n",
    "# print(type(y_pred), len(y_pred), y_pred)\n",
    "# print(type(y_test), len(y_test), y_test)\n",
    "\n",
    "naive_bayes_accuracy = accuracy_score(y_test, y_pred)\n",
    "naive_bayes_precision = precision_score(y_test, y_pred, average='macro')\n",
    "naive_bayes_recall = recall_score(y_test, y_pred, average='macro')\n",
    "naive_bayes_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {naive_bayes_accuracy}')\n",
    "print(f'precision: {naive_bayes_precision}')\n",
    "print(f'Recall: {naive_bayes_recall}')\n",
    "print(f'F1 Score: {naive_bayes_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用K近邻算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print('使用K近邻模型')\n",
    "\n",
    "K_Nearest_model = KNeighborsClassifier()\n",
    "K_Nearest_model.fit(train_vec, y_train)\n",
    "\n",
    "joblib.dump(K_Nearest_model, 'K-Nearest.pkl', compress=3)\n",
    "y_pred = K_Nearest_model.predict(test_vec)\n",
    "# print(type(y_pred), len(y_pred), y_pred)\n",
    "# print(type(y_test), len(y_test), y_test)\n",
    "\n",
    "K_Nearest_accuracy = accuracy_score(y_test, y_pred)\n",
    "K_Nearest_precision = precision_score(y_test, y_pred, average='macro')\n",
    "K_Nearest_recall = recall_score(y_test, y_pred, average='macro')\n",
    "K_Nearest_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {K_Nearest_accuracy}')\n",
    "print(f'precision: {K_Nearest_precision}')\n",
    "print(f'Recall: {K_Nearest_recall}')\n",
    "print(f'F1 Score: {K_Nearest_f1}')\n",
    "print(K_Nearest_model.predict_proba(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习方法进行判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Voting软投票的方式去集成学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_vec)\n",
    "X_test = scaler.transform(test_vec)\n",
    "\n",
    "# 定义基础模型\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svc = SVC(probability=True, kernel='linear', random_state=42)\n",
    "\n",
    "# 定义 VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knn', knn), ('rf', rf), ('svc', svc)],\n",
    "    #estimators=[('knn', knn), ('rf', rf)],\n",
    "    #estimators=[('knn', knn), ('svc', svc)],\n",
    "    #estimators=[('rf', rf), ('svc', svc)],\n",
    "    voting='soft'  # 使用软投票（基于概率的投票）\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(voting_clf, 'Voting_model.pkl', compress=3)\n",
    "\n",
    "# 评估模型\n",
    "Voting_accuracy = accuracy_score(y_test, y_pred)\n",
    "Voting_precision = precision_score(y_test, y_pred, average='macro')\n",
    "Voting_recall = recall_score(y_test, y_pred, average='macro')\n",
    "Voting_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {Voting_accuracy}')\n",
    "print(f'precision: {Voting_precision}')\n",
    "print(f'Recall: {Voting_recall}')\n",
    "print(f'F1 Score: {Voting_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Stacking的方式去集成学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_vec)\n",
    "X_test = scaler.transform(test_vec)\n",
    "\n",
    "# 定义基础模型\n",
    "estimators = [\n",
    "    #('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(probability=True, kernel='linear', random_state=42))\n",
    "]\n",
    "\n",
    "# 定义元学习器\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(stacking_clf, 'Stacking_model.pkl', compress=3)\n",
    "\n",
    "# 评估模型\n",
    "Stacking_accuracy = accuracy_score(y_test, y_pred)\n",
    "Stacking_precision = precision_score(y_test, y_pred, average='macro')\n",
    "Stacking_recall = recall_score(y_test, y_pred, average='macro')\n",
    "Stacking_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {Stacking_accuracy}')\n",
    "print(f'precision: {Stacking_precision}')\n",
    "print(f'Recall: {Stacking_recall}')\n",
    "print(f'F1 Score: {Stacking_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict(line, word_model, predict_model_name):\n",
    "    vec = np.zeros(64).reshape((1, 64))\n",
    "    count = 0\n",
    "    for word in jieba.cut(line, cut_all=False):\n",
    "        try:\n",
    "            vec = vec + word_model.wv[word].reshape((1, 64))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    clf = joblib.load(filename=predict_model_name)\n",
    "\n",
    "    result = clf.predict(vec)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附件名检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def is_executable_or_script(filename):\n",
    "\n",
    "    windows_extensions = [\n",
    "        \".exe\", \".bat\", \".cmd\", \".msi\", \".com\", \".vbs\", \".ps1\", \".wsf\", \".scr\", \".cpl\"\n",
    "    ]\n",
    "    \n",
    "    unix_extensions = [\n",
    "        \".sh\", \".bash\", \".bin\", \".run\", \".out\", \".py\", \".pl\", \".php\", \".rb\", \n",
    "        \".js\", \".cgi\", \".ksh\", \".zsh\", \".tcl\", \".lua\", \".groovy\", \".r\", \".awk\"\n",
    "    ]\n",
    "    \n",
    "    cross_platform_extensions = [\n",
    "        \".jar\", \".class\", \".pyc\", \".dll\", \".so\", \".tar.gz\", \".deb\", \".rpm\", \".pkg\", \".dmg\"\n",
    "    ]\n",
    "    \n",
    "    _, ext = os.path.splitext(filename)\n",
    "    \n",
    "    if ext.lower() in windows_extensions + unix_extensions + cross_platform_extensions:\n",
    "        return True, f\"文件名以'{ext}'结尾，这是一个常见的脚本或可执行文件扩展名。\"\n",
    "    else:\n",
    "        if os.name != 'nt' and os.access(filename, os.X_OK):\n",
    "            return True, \"文件在Unix/Linux系统中有执行权限，可能是一个可执行文件或脚本。\"\n",
    "    \n",
    "    return False, \"文件没有已知的可执行或脚本文件扩展名，且在Unix/Linux系统中没有执行权限。\"\n",
    "\n",
    "filename = \"example.out\"\n",
    "is_script, reason = is_executable_or_script(filename)\n",
    "print(is_script)  \n",
    "print(reason)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附件名相关性检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def attach_text_similarity(text, accachment):\n",
    "    count = 0\n",
    "    for word in jieba.cut(accachment, cut_all=True):\n",
    "        if word in jieba.cut(text, cut_all=True):\n",
    "            count += 1\n",
    "    \n",
    "    similarity = count / len(list(jieba.cut(accachment, cut_all=True)))\n",
    "\n",
    "    return similarity >= 0.5, similarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('test.model')\n",
    "print(attach_text_similarity('你好，这些是你的成绩单', '成绩报告'))\n",
    "print(class_predict('华源集团公司作为当今高科技产品--  笔记本电脑、时尚手机、电脑硬件、数码', model, 'Voting_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# 示例文本\n",
    "text = \"这是一个包含URL的文本，请访问 https://www.example.com 获取更多信息，或者访问 http://test.com 进行测试。\"\n",
    "\n",
    "# 提取URL\n",
    "urls = extract_urls(text)\n",
    "\n",
    "# 打印提取的URL\n",
    "print(\"提取到的URLs:\", urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EML文件解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_eml_details(eml_file_path):\n",
    "    with open(eml_file_path, 'rb') as f:\n",
    "        msg = BytesParser(policy=policy.default).parse(f)\n",
    "\n",
    "    sender = msg['From']\n",
    "    sender_email = re.search(r'[\\w\\.-]+@[\\w\\.-]+', sender).group(0)\n",
    "\n",
    "    received_headers = msg.get_all('Received', [])\n",
    "    ip_address = None\n",
    "    for header in received_headers:\n",
    "        ip_match = re.search(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b', header)\n",
    "        if ip_match:\n",
    "            ip_address = ip_match.group(0)\n",
    "            break\n",
    "\n",
    "    body_content = None\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.iter_parts():\n",
    "            content_type = part.get_content_type()\n",
    "            if content_type == 'text/plain':\n",
    "                body_content = part.get_payload(decode=True).decode(part.get_content_charset(), errors='replace')\n",
    "                break\n",
    "            elif content_type == 'text/html':\n",
    "                html_content = part.get_payload(decode=True).decode(part.get_content_charset(), errors='replace')\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                body_content = soup.get_text()\n",
    "                break\n",
    "    else:\n",
    "        content_type = msg.get_content_type()\n",
    "        if content_type == 'text/plain':\n",
    "            body_content = msg.get_payload(decode=True).decode(msg.get_content_charset(), errors='replace')\n",
    "        elif content_type == 'text/html':\n",
    "            html_content = msg.get_payload(decode=True).decode(msg.get_content_charset(), errors='replace')\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            body_content = soup.get_text()\n",
    "\n",
    "    attachment_filenames = []\n",
    "    for part in msg.iter_attachments():\n",
    "        filename = part.get_filename()\n",
    "        if filename:\n",
    "            attachment_filenames.append(filename)\n",
    "\n",
    "    return {\n",
    "        'sender_email': sender_email,\n",
    "        'ip_address': ip_address,\n",
    "        'body_content': body_content,\n",
    "        'url_list': extract_urls(body_content),\n",
    "        'attachment_filenames': attachment_filenames,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
